---
layout: single
title:  "Ensemble(앙상블)"
categories: 머신러닝
tag: [머신러닝, python, Ensemble, 결정트리, 앙상블]
toc: true
toc_sticky: true

---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>

## Decision Tree Ensemble(결정트리 앙상블)

- 앙상블(ensemble)은 **여러 머신러닝 모델을 연결**하여 더 강력한 모델을 만드는 기법
- 결정트리의 가지치기 후 **과대적합되는 단점을 보완**하는 모델
- **RandomForest, GradientBoosting, XGBoost, LightGBM, CatBoost** 등
- 회귀와 분류에 모두 사용 가능

## 앙상블 기법

1. **보팅(Voting)**
    - 서로 다른 모델을 학습시켜 예측 결과를 **다수의 결과로 투표**하는 방법
    - 앙상블의 가장 기본 원리
    - **하드 보팅**(최종 예측 결과로만 보팅), **소프트 보팅**(모델들의 예측 확률로 보팅)으로 나뉜다.
2. **배깅(Bagging)**
    - 같은 알고리즘을 가진 모델들을 학습시켜 예측 결과를 투표하는 방법
    - 학습 데이터(행)를 모델마다 다르게 샘플링(Bootstraping), 병렬 학습
    - 학습에 사용되는 컬럼(열)을 다르게 랜덤 선택
    - 배깅이 적용된 Decision Tree가 **RandomForest**
3. **부스팅**
    - 같은 알고리즘을 가진 모델을 학습시켜 예측결과를 투표하는 방법
    - **이전 모델이 잘못 예측한 부분을 다음 모델이 강조해서 학습**하는 방법
    - 순차적 학습 -> 학습 속도가 비교적 느림

## RandomForest (배깅 모델)

- 서로 다른 방향으로 **과대적합된 트리를 많이 만들고 평균을 내어 일반화** 시키는 모델
- 다양한 트리를 만드는 방법 두 가지
    - 트리를 만들 때 사용하는 데이터 포인트 샘플을 무작위로 선택한다.
    - 노드 구성시 기준이 되는 특성을 무작위로 선택하게 한다.
- **결정트리의 단점을 보완**하고 **장점은 그대로** 가지고 있는 모델이어서 별다른 조정 없이도 괜찮을 결과를 만들어낸다.
- 트리가 여러 개 만들어지기 때문에 비전문가에게 예측과정을 보여주기는 어렵다.
- 랜덤하게 만들어지기 때문에 random_state를 고정해야 같은 결과를 볼 수 있다.

### 주요 매개변수

- n_estimators : 생성할 트리의 갯수
- n개의 데이터 부트스트랩 샘플 구성 (n개의 데이터 포인트 중 무작위로 n 횟수만큼 반복 추출, 중복된 데이터가 들어 있을 수 있다.)
- max_features : 무작위로 선택될 후보 특성의 개수 (각 노드 별로 max_features 개수 만큼 무작위로 특성을 고른 뒤 최선의 특성을 찾는다.)
- max_features를 높이면 트리들이 비슷해진다.

```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=923,    
                                  n_estimators=1000,   # 앙상블할 모델(tree)의 갯수
                                  max_features=0.6,   # 학습에 사용할 특성(컬럼) 수 (설정된 값은 60%)
                                  max_depth=15        # 트리의 깊이 설정
                                 )
```

## GradientBoosting (부스팅 모델)

- 정확도가 낮더라도 얕은 깊이의 모델을 만든 뒤, 나타난 예측 오류를 두 번째 모델이 보완한다.
- **이전 트리의 예측 오류를 보완하여 다음 트리를 만드는 작업을 반복**한다.
- 마지막까지 **성능을 쥐어짜고 싶은 경우** 사용한다, 주로 경진 대회에서 많이 활용.
(GradientBoosting을 더 발전시킨 **XGBoost**도 있음)
- 보통 트리의 깊이를 깊게하지 않기 때문에 **예측 속도는 비교적 빠르다**. 하지만 이전 트리의 오차를 반영해서 새로운 트리를 만들기 때문에 **학습속도가 느리다**.
- 특성의 **스케일을 조정하지 않아도 된다.**

### 주요 매개변수

- n_estimators : 생성할 트리의 개수 (트리가 많아질수록 과대적합이 될 수 있다.)
- learning_rate : 오차를 보정하는 정도 (값이 높을수록 오차를 많이 보정하려고 한다.)
- max_depth : 트리의 깊이 (일반적으로 **트리의 깊이를 깊게 설정하지 않는다.**)

```python
from sklearn.ensemble import GradientBoostingClassifier

gbtree = GradientBoostingClassifier(n_estimators=100,    # 트리의 갯수(순차적으로 생성되고, 이전 트리의 오류에 가중치가 더해짐)
                                   learning_rate=0.1,    # 이전트리의 오류를 다음트리에 얼마나 반영할지
                                   max_depth=5,          # 트리의 깊이
                                   subsample=1           # 1: 전체데이터 사용, 과적합이 우려된다면 값을 줄여도 됨
                                   )
```